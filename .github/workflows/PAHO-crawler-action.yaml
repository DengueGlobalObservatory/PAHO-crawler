name: PAHO web crawling on Github Action
on:
  workflow_dispatch:
  schedule:
    - cron: '00 08 * * *' # 08:00 am UTC every day

jobs:
  Scraping_data:
    runs-on: ubuntu-latest
    steps:
      - name: Checking out repo
        uses: actions/checkout@v3
      - name: Setting up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.12.3'
      - name: Install Google Chrome Stable
        run: |
              echo "Starting Chrome installation..."
              sudo apt-get update -y
              # Attempt to install dependencies that Chrome might need
              sudo apt-get install -y --no-install-recommends \
                libasound2 \
                libatk-bridge2.0-0 \
                libatk1.0-0 \
                libc6 \
                libcairo2 \
                libcups2 \
                libdbus-1-3 \
                libexpat1 \
                libfontconfig1 \
                libgbm1 \
                libgcc1 \
                libglib2.0-0 \
                libgtk-3-0 \
                libnspr4 \
                libnss3 \
                libpango-1.0-0 \
                libpangocairo-1.0-0 \
                libstdc++6 \
                libx11-6 \
                libx11-xcb1 \
                libxcb1 \
                libxcomposite1 \
                libxcursor1 \
                libxdamage1 \
                libxext6 \
                libxfixes3 \
                libxi6 \
                libxrandr2 \
                libxrender1 \
                libxss1 \
                libxtst6 \
                lsb-release \
                wget \
                xdg-utils
              # Download and install Chrome
              wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
              sudo dpkg -i google-chrome-stable_current_amd64.deb
              # Fix any broken dependencies that might occur after dpkg
              sudo apt-get install -f -y
              google-chrome-stable --version # Verify installation
              echo "Chrome installation finished."

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install selenium
          pip install undetected_chromedriver
          pip install setuptools
          pip install webdriver_manager

      - name: Create data directory
        run: mkdir -p ${{ github.workspace }}/data


      - name: Running the Python script
        run: python scripts/PAHOCrawler_UCver_v2.py

      - name: Commit and Push
        run: |
         git config --global user.name "github-actions[bot]"
         git config --global user.email "41898282+github-actions[bot]@users.noreply.github.com"
         git add -A
         timestamp=$(date -u)
         git commit -am "Latest data: ${timestamp}" || exit 0
         git push

      - name: Upload Scraped Data Artifacts
        if: always() # Run this step even if the script fails, to get partial data or logs
        uses: actions/upload-artifact@v4
        with:
          name: paho-scraped-data-${{ github.run_id }}
          path: | # Use multi-line path to include multiple locations
            ./data_output/ # Path to the folder where your script saves final CSV files
            ./*.png # Upload any PNG files (screenshots for errors) from the workspace root
            ./temp_chrome_downloads/ # Optional: upload temporary downloads if needed for debugging
          retention-days: 7 # Optional: how long to keep artifacts

